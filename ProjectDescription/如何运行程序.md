## 必要步骤,请按顺序执行
1. 先进入docker目录，运行下面的命令启动容器
```bash
docker-compose up -d
```
2. 在 backend-AI/model/Qwen 目录下执行下面的命令下载模型参数，模型参数大小约为8G
```bash
git clone https://www.modelscope.cn/Qwen/Qwen3-Embedding-4B.git
```
3. 在 backend-AI/model 执行下面的命令下载模型参数
```bash
git clone https://github.com/index-tts/index-tts.git
git clone https://www.modelscope.cn/iic/SenseVoiceSmall.git
```
index-tts需要按照其readme配置环境，安装相关依赖。
4. 注册一个 ModelScope 账号，获得 API key 用于调用 LLM 模型或 MLLM 模型。

5. 根据ModelScope文档，配置Conda和Pytorch环境。在backen-AI/model目录下开三个终端运行
```bash
python3 Qwen-LLM_API.py
python3 Qwen3-Embedding-4B_API.py
python3 SenseVoice_API.py
```

6. 进入下载好的index-tts，在index-tts目录下运行
```bash
uv run tts_API.py
```

7. 在 backend-AI/flask_api 目录下运行下面的指令
```bash
python3 basic_API.py
```
如果缺什么依赖就装什么，这个不是很难。

8. 在 frontend 目录下运行下面的指令
```bash
npm i
npm run dev
```
运行完 npm run dev就会给出一个ip端口号，在浏览器输入它就可以访问。

注意，这是生产环境的指令，不是开发环境的指令。执行完上述步骤，如果有报错，再检查一下各个py程序中的服务地址。如果上述微服务都部署在一个机子上，没什么大问题。

如果不在一个机子上，若无法访问需要考虑云服务安全组出入规则。

## 可选步骤（后续开发）
1. 由于Qwen3-VL系列的免费推理API只支持图片URL形式。因此需要一个公网资源服务器。这是file_server的作用，对应的云服务是对象存储。
2. backend-AI/model下还有一个Qwen-VL_API.py，是后续多模态语音聊天的扩展。